---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - "/wordpress/"
  - "/wordpress/index.html"
---

{% include base_path %}

   
Welcome to my homepage! My full name is Duc-Minh Le, you can call me Minh üëã. I am currently an AI Engineer at Trivita AI. Previously, I was a research resident at [Qualcomm AI Research](https://www.qualcomm.com/research/artificial-intelligence) ü§ñ, where I had the privilege of being advised by Professor [Nhat Ho](https://nhatptnk8912.github.io/) üèõÔ∏è. I earned my Bachelor's degree in Computer Science from [Hanoi University of Science and Technology](https://hust.edu.vn/). 


Email: minh611002@gmail.com
## Research Interests 


My research focuses on advancing Artificial Intelligence and Machine Learning, with a strong emphasis on making them scalable and efficient ‚¨Ü. I've explored exciting topics such as Parameter-Efficient Fine-Tuning for optimizing AI models and Mixture of Experts for enhanced scalability. Additionally, I've worked on Continual Learning and remain deeply curious about new ideas and interdisciplinary research.

I'm actively seeking PhD positions in Computer Science for the upcoming academic year and excited to collaborate on impactful research! üöÄ


<span style="color:red"> **(\*) denotes equal contribution.** </span> <br/>

## Selected Preprints

[One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning](https://arxiv.org/abs/2509.24483) <br/>
Under review <br/>
*__Minh Le__, Bao-Ngoc Dao, Huy Nguyen, Quyen Tran, Anh Nguyen, Nhat Ho*<br/>

[On the Expressiveness of Visual Prompt Experts](https://arxiv.org/abs/2501.18936) <br/>
Under review <br/>
*__Minh Le\*__, Anh Nguyen\*, Huy Nguyen, Chau Nguyen, Anh Tran, Nhat Ho*<br/>

[Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944) <br/>
Under review <br/>
Bao-Ngoc Dao\*, Quang Nguyen\*, Luyen Ngo Dinh\*, __Minh Le\*__, Nam Le, Linh Ngo Van<br/>

[Leveraging Hierarchical Taxonomies in Prompt-based Continual Learning](https://arxiv.org/abs/2410.04327) <br/>
Under review <br/>
*Quyen Tran, Hoang Phan\*, __Minh Le__\*, Tuan Truong, Dinh Phung, Linh Ngo, Thien Nguyen, Nhat Ho, Trung Le*<br/>


## Selected Publications on Mixture of Experts
[RepLoRA: Reparameterizing Low-rank Adaptation via the Perspective of Mixture of Experts](https://arxiv.org/abs/2502.03044) <br/> 
Proceedings of the ICML, 2025 <br/>
*Tuan Truong\*, Chau Nguyen\*, Huy Nguyen\*, __Minh Le__, Trung Le, Nhat Ho*<br/>

[On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation](https://arxiv.org/abs/2502.03029) <br/>
Proceedings of the ICML, 2025 <br/>
*Nghiem T. Diep\*, Huy Nguyen\*, Chau Nguyen\*, __Minh Le__, Duy M. H. Nguyen, Daniel Sonntag, Mathias Niepert, Nhat Ho*<br/>

[Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts](https://arxiv.org/abs/2410.02200) <br/>
Proceedings of the ICLR, 2025 <br/>
*__Minh Le\*__, Chau Nguyen\*, Huy Nguyen\*, Quyen Tran, Trung Le, Nhat Ho*<br/>

[Adaptive Prompting for Continual Relation Extraction: A Within-Task Variance Perspective](https://arxiv.org/abs/2412.08285) <br/>
Proceedings of the AAAI Conference on Artificial Intelligence 39, 2025 <span style="color:red"> **(Oral)** </span> <br/>
*__Minh Le\*__, Tien Ngoc Luu\*, An Nguyen The\*, Thanh-Thien Le, Trang Nguyen, Thanh Tung Nguyen, Linh Ngo Van, Thien Huu Nguyen*<br/>

[Mixture of Experts Meets Prompt-Based Continual Learning](https://arxiv.org/abs/2405.14124) <br/>
Advances in NeurIPS, 2024 <br/>
*__Minh Le__, An Nguyen\*, Huy Nguyen\*, Trang Nguyen\*, Trang Pham\*, Linh Van Ngo, Nhat Ho*<br/>



<!-- *__Huy Nguyen__, Nhat Ho, Alessandro Rinaldo*<br/>
Proceedings of the ICML, 2024. [[arXiv](https://arxiv.org/abs/2402.02952)]
### Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?
*__Huy Nguyen__, Pedram Akbarian, Nhat Ho*<br/>
Proceedings of the ICML, 2024. [[arXiv](https://arxiv.org/abs/2401.13875)]
### Demystifying Softmax Gating Function in Gaussian Mixture of Experts 
*__Huy Nguyen__, TrungTin Nguyen, Nhat Ho*<br/>
Advances in NeurIPS, 2023  <span style="color:red"> **(Spotlight)** </span>. [[arXiv](https://arxiv.org/abs/2305.03288)] [[NeurIPS](https://openreview.net/pdf?id=cto6jIIbMZ)]
### Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts
*__Huy Nguyen__, Pedram Akbarian, Fanqi Yan, Nhat Ho*<br/>
Proceedings of the ICLR, 2024. [[arXiv](https://arxiv.org/abs/2309.13850)]
### A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
*__Huy Nguyen__, Pedram Akbarian, TrungTin Nguyen, Nhat Ho*<br/>
Proceedings of the ICML, 2024. [[arXiv](https://arxiv.org/abs/2310.14188)]
### Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts
*__Huy Nguyen\*__, TrungTin Nguyen\*, Khai Nguyen, Nhat Ho*<br/>
In AISTATS, 2024. [[arXiv](https://arxiv.org/abs/2305.07572)]
### On Parameter Estimation in Deviated Gaussian Mixture of Experts
*__Huy Nguyen__, Khai Nguyen, Nhat Ho*<br/>
In AISTATS, 2024. [[arXiv](https://arxiv.org/abs/2402.05220)] -->

<!-- ## Selected Publications on Optimal Transport
### Entropic Gromov-Wasserstein between Gaussian Distributions
*__Huy Nguyen\*__, Khang Le\*, Dung Le\*, Dat Do, Tung Pham, Nhat Ho*<br/>
Proceedings of the ICML, 2022.  [[arXiv](https://arxiv.org/abs/2108.10961)] [[ICML](https://proceedings.mlr.press/v162/le22a.html)]
### On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity
*__Huy Nguyen\*__, Khang Le\*, Khai Nguyen, Tung Pham, Nhat Ho*<br/>
In AISTATS, 2022.  [[arXiv](https://arxiv.org/abs/2108.07992)] [[AISTATS](https://proceedings.mlr.press/v151/le22a.html)]
### On Robust Optimal Transport: Computational Complexity and Barycenter Computation
*__Huy Nguyen\*__, Khang Le\*, Quang Minh Nguyen, Tung Pham, Hung Bui, Nhat Ho*<br/>
Advances in NeurIPS, 2021.  [[arXiv](https://arxiv.org/abs/2102.06857)] [[NeurIPS](https://proceedings.neurips.cc/paper/2021/hash/b80ba73857eed2a36dc7640e2310055a-Abstract.html)]



## Recent News
- **[May 2024]** Three new papers on Mixture of Experts [[1](https://arxiv.org/abs/2405.13997)], [[2](https://arxiv.org/abs/2405.14131)] and [[3](https://arxiv.org/abs/2405.14124)] are out!
- **[May 2024]** Three papers on Mixture of Experts, [[1](https://arxiv.org/abs/2402.02952)], [[2](https://arxiv.org/abs/2401.13875)] and [[3](https://arxiv.org/abs/2310.14188)], are accepted to ICML 2024.
- **[Apr 2024]** I was offered the AISTATS 2024 registration grant. See you in Valencia, Spain this May!
- **[Mar 2024]** I received the ICLR 2024 Travel Award. See you in Vienna, Austria this May!
- **[Feb 2024]** Two new papers on the applications of Mixture of Experts in Medical Images [[1](https://arxiv.org/abs/2402.03226)] and Large Language Models [[2](https://arxiv.org/abs/2402.02526)] are out!
- **[Feb 2024]** Two new papers on the theory of Mixture of Experts, [[1](https://arxiv.org/abs/2402.02952)] and [[2](https://arxiv.org/abs/2401.13875)], are out! 
- **[Jan 2024]** Two papers on Mixture of Experts, [[1](https://arxiv.org/abs/2305.07572)] and [[2](https://arxiv.org/abs/2402.05220)], are accepted to AISTATS 2024.
- **[Jan 2024]** Our paper "[Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://arxiv.org/abs/2309.13850)" is accepted to ICLR 2024.
- **[Dec 2023]** Our paper "[Fast Approximation of the Generalized Sliced-Wasserstein Distance](https://openreview.net/forum?id=u3JeFO8G8s)" is accepted to ICASSP 2024.
- **[Oct 2023]** I received the NeurIPS 2023 Scholar Award. See you in New Orleans this December!
- **[Oct 2023]** Our new paper "[A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts](https://arxiv.org/pdf/2310.14188.pdf)" is out.
- **[Sep 2023]** Our new paper "[Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://arxiv.org/pdf/2309.13850.pdf)" is out.
- **[Sep 2023]** We have two papers accepted to NeurIPS 2023, [[1](https://arxiv.org/pdf/2305.03288.pdf)] as <span style="color:red"> **spotlight** </span> and [[2](https://arxiv.org/pdf/2301.11808.pdf)] as poster.
- **[Jul 2023]** We will present the paper "[Fast Approximation of the Generalized Sliced-Wasserstein Distance](https://openreview.net/pdf?id=u3JeFO8G8s)" at the Frontier4LCD workshop, ICML 2023.
- **[May 2023]** Three new papers on the Mixture of Experts theory are out! See more at [[1]](https://arxiv.org/abs/2305.03288), [[2]](https://arxiv.org/abs/2305.07572) and [[3](https://huynm99.github.io/Deviated_MoE.pdf)].
- **[Feb 2023]** Our new paper on Mixture Models theory "[Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models](https://arxiv.org/abs/2301.11808)" is out. -->

## Professional Services

Conference Reviewer: ICML 2025, NeurIPS 2025, ICLR 2026
