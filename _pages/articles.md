---
permalink: /articles/
title: "Publications"
excerpt: "Publications"
author_profile: true
redirect_from: 
  - "/wordpress/"
  - "/wordpress/index.html"
---

{% include base_path %}

(*) denotes equal contribution


## Preprints
**[P.4]** [One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning](https://arxiv.org/abs/2509.24483). <br/>
*Under review*. <br/>
<b>Minh Le</b>, Bao-Ngoc Dao, Huy Nguyen, Quyen Tran, Anh Nguyen, Nhat Ho

**[P.3]** [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944). <br/>
*Under review*. <br/>
Bao-Ngoc Dao\*, Quang Nguyen\*, Luyen Ngo Dinh\*, <b>Minh Le\*</b>, Nam Le, Linh Ngo Van

**[P.2]** [On the Expressiveness of Visual Prompt Experts](https://arxiv.org/abs/2501.18936). <br/>
*Under review*. <br/>
<b>Minh Le\*</b>, Anh Nguyen\*, Huy Nguyen, Chau Nguyen, Anh Tran, Nhat Ho

**[P.1]** [Leveraging Hierarchical Taxonomies in Prompt-based Continual Learning](https://arxiv.org/abs/2410.04327). <br/>
*Under review*. <br/>
Quyen Tran, Hoang Phan\*, <b>Minh Le\*</b>, Tuan Truong, Dinh Phung, Linh Ngo, Thien Nguyen, Nhat Ho, Trung Le




<!-- **[P.2]** [Fuse MoE: Mixture-of-Experts Transformers for Fleximodal Fusion](https://arxiv.org/pdf/2402.03226.pdf). *Under review*. <br/>
Xing Han, <b>Huy Nguyen\*</b>, Carl Harris\*, Nhat Ho, Suchi Saria

**[P.1]** [CompeteSMoE - Effective Training of Sparse Mixture of Experts via Competition](https://arxiv.org/pdf/2402.02526.pdf). *Under review*. <br/>
Quang Pham, Giang Do, <b>Huy Nguyen</b>, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, Nhat Ho -->

## Conference Publications


**[C.5]** [RepLoRA: Reparameterizing Low-rank Adaptation via the Perspective of Mixture of Experts](https://arxiv.org/abs/2502.03044). <br/>
*Proceedings of the ICML, 2025*. <br/>
Tuan Truong\*, Chau Nguyen\*, Huy Nguyen\*, <b>Minh Le</b>, Trung Le, Nhat Ho

**[C.4]** [On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation](https://arxiv.org/abs/2502.03029). <br/>
*Proceedings of the ICML, 2025*. <br/>
Nghiem T. Diep\*, Huy Nguyen\*, Chau Nguyen\*, <b>Minh Le</b>, Duy M. H. Nguyen, Daniel Sonntag, Mathias Niepert, Nhat Ho

**[C.3]** [Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts](https://arxiv.org/pdf/2410.02200). <br/>
*Proceedings of the ICLR, 2025*. <br/>
<b>Minh Le\*</b>, Chau Nguyen\*, Huy Nguyen\*, Quyen Tran, Trung Le, Nhat Ho

**[C.2]** [Adaptive Prompting for Continual Relation Extraction: A Within-Task Variance Perspective](https://arxiv.org/abs/2412.08285). <br/>
*Proceedings of the AAAI Conference on Artificial Intelligence 39, 2025* <span style="color:red"> **(Oral)** </span>. <br/> 
<b>Minh Le\*</b>, Tien Ngoc Luu\*, An Nguyen The\*, Thanh-Thien Le, Trang Nguyen, Thanh Tung Nguyen, Linh Ngo Van, Thien Huu Nguyen

**[C.1]** [Mixture of Experts Meets Prompt-Based Continual Learning](https://arxiv.org/pdf/2405.14124.pdf). <br/>
*Advances in NeurIPS, 2024*. <br/>
<b>Minh Le</b>, An Nguyen\*, Huy Nguyen\*, Trang Nguyen\*, Trang Pham\*, Linh Van Ngo, Nhat Ho